{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ZEivC8e8GT",
        "outputId": "5759d118-685b-42c3-c8bc-4eeacdeefe95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "#@title import efficientvim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.layers import SqueezeExcite\n",
        "\n",
        "\n",
        "class LayerNorm2D(nn.Module):\n",
        "    \"\"\"LayerNorm for channels of 2D tensor(B C H W)\"\"\"\n",
        "    def __init__(self, num_channels, eps=1e-5, affine=True):\n",
        "        super(LayerNorm2D, self).__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
        "            self.bias = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=1, keepdim=True)  # (B, 1, H, W)\n",
        "        var = x.var(dim=1, keepdim=True, unbiased=False)  # (B, 1, H, W)\n",
        "\n",
        "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)  # (B, C, H, W)\n",
        "\n",
        "        if self.affine:\n",
        "            x_normalized = x_normalized * self.weight + self.bias\n",
        "\n",
        "        return x_normalized\n",
        "\n",
        "\n",
        "class LayerNorm1D(nn.Module):\n",
        "    \"\"\"LayerNorm for channels of 1D tensor(B C L)\"\"\"\n",
        "    def __init__(self, num_channels, eps=1e-5, affine=True):\n",
        "        super(LayerNorm1D, self).__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.ones(1, num_channels, 1))\n",
        "            self.bias = nn.Parameter(torch.zeros(1, num_channels, 1))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=1, keepdim=True)  # (B, 1, H, W)\n",
        "        var = x.var(dim=1, keepdim=True, unbiased=False)  # (B, 1, H, W)\n",
        "\n",
        "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)  # (B, C, H, W)\n",
        "\n",
        "        if self.affine:\n",
        "            x_normalized = x_normalized * self.weight + self.bias\n",
        "\n",
        "        return x_normalized\n",
        "\n",
        "\n",
        "class ConvLayer2D(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, norm=nn.BatchNorm2d, act_layer=nn.ReLU, bn_weight_init=1):\n",
        "        super(ConvLayer2D, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_dim,\n",
        "            out_dim,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            stride=(stride, stride),\n",
        "            padding=(padding, padding),\n",
        "            dilation=(dilation, dilation),\n",
        "            groups=groups,\n",
        "            bias=False\n",
        "        )\n",
        "        self.norm = norm(num_features=out_dim) if norm else None\n",
        "        self.act = act_layer() if act_layer else None\n",
        "\n",
        "        if self.norm:\n",
        "            torch.nn.init.constant_(self.norm.weight, bn_weight_init)\n",
        "            torch.nn.init.constant_(self.norm.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvLayer1D(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, norm=nn.BatchNorm1d, act_layer=nn.ReLU, bn_weight_init=1):\n",
        "        super(ConvLayer1D, self).__init__()\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_dim,\n",
        "            out_dim,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "            groups=groups,\n",
        "            bias=False\n",
        "        )\n",
        "        self.norm = norm(num_features=out_dim) if norm else None\n",
        "        self.act = act_layer() if act_layer else None\n",
        "\n",
        "        if self.norm:\n",
        "            torch.nn.init.constant_(self.norm.weight, bn_weight_init)\n",
        "            torch.nn.init.constant_(self.norm.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, in_dim, dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = ConvLayer2D(in_dim, dim, 1)\n",
        "        self.fc2 = ConvLayer2D(dim, in_dim, 1, act_layer=None, bn_weight_init=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc2(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self,  in_dim=3, dim=96):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvLayer2D(in_dim, dim // 8, kernel_size=3, stride=2, padding=1),\n",
        "            ConvLayer2D(dim // 8, dim // 4, kernel_size=3, stride=2, padding=1),\n",
        "            ConvLayer2D(dim // 4, dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "            ConvLayer2D(dim // 2, dim, kernel_size=3, stride=2, padding=1, act_layer=None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self,  in_dim, out_dim, ratio=4.0):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(out_dim * ratio)\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvLayer2D(in_dim, hidden_dim, kernel_size=1),\n",
        "            ConvLayer2D(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, groups=hidden_dim),\n",
        "            SqueezeExcite(hidden_dim, .25),\n",
        "            ConvLayer2D(hidden_dim, out_dim, kernel_size=1, act_layer=None)\n",
        "        )\n",
        "\n",
        "        self.dwconv1 = ConvLayer2D(in_dim, in_dim, 3, padding=1, groups=in_dim, act_layer=None)\n",
        "        self.dwconv2 = ConvLayer2D(out_dim, out_dim, 3, padding=1, groups=out_dim, act_layer=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dwconv1(x)\n",
        "        x = self.conv(x)\n",
        "        x = x + self.dwconv2(x)\n",
        "        return x\n",
        "\n",
        "!pip install fvcore\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from timm.layers import trunc_normal_\n",
        "from timm.models import register_model\n",
        "from fvcore.nn import flop_count\n",
        "\n",
        "class HSMSSD(nn.Module):\n",
        "    def __init__(self, d_model, ssd_expand=1, A_init_range=(1, 16), state_dim = 64):\n",
        "        super().__init__()\n",
        "        self.ssd_expand = ssd_expand\n",
        "        self.d_inner = int(self.ssd_expand * d_model)\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "        self.BCdt_proj = ConvLayer1D(d_model, 3*state_dim, 1, norm=None, act_layer=None)\n",
        "        conv_dim = self.state_dim*3\n",
        "        self.dw = ConvLayer2D(conv_dim, conv_dim, 3,1,1, groups=conv_dim, norm=None, act_layer=None, bn_weight_init=0)\n",
        "        self.hz_proj = ConvLayer1D(d_model, 2*self.d_inner, 1, norm=None, act_layer=None)\n",
        "        self.out_proj = ConvLayer1D(self.d_inner, d_model, 1, norm=None, act_layer=None, bn_weight_init=0)\n",
        "\n",
        "        A = torch.empty(self.state_dim, dtype=torch.float32).uniform_(*A_init_range)\n",
        "        self.A = torch.nn.Parameter(A)\n",
        "        self.act = nn.SiLU()\n",
        "        self.D = nn.Parameter(torch.ones(1))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, _, L= x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "\n",
        "        BCdt = self.dw(self.BCdt_proj(x).view(batch,-1, H, H)).flatten(2)\n",
        "        B,C,dt = torch.split(BCdt, [self.state_dim, self.state_dim,  self.state_dim], dim=1)\n",
        "        A = (dt + self.A.view(1,-1,1)).softmax(-1)\n",
        "\n",
        "        AB = (A * B)\n",
        "        h = x @ AB.transpose(-2,-1)\n",
        "\n",
        "        h, z = torch.split(self.hz_proj(h), [self.d_inner, self.d_inner], dim=1)\n",
        "        h = self.out_proj(h * self.act(z)+ h * self.D)\n",
        "        y = h @ C # B C N, B C L -> B C L\n",
        "\n",
        "        y = y.view(batch,-1,H,H).contiguous()# + x * self.D  # B C H W\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class EfficientViMBlock(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4., ssd_expand=1, state_dim=64):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        self.mixer = HSMSSD(d_model=dim, ssd_expand=ssd_expand,state_dim=state_dim)\n",
        "        self.norm = LayerNorm1D(dim)\n",
        "\n",
        "        self.dwconv1 = ConvLayer2D(dim, dim, 3, padding=1, groups=dim, bn_weight_init=0, act_layer = None)\n",
        "        self.dwconv2 = ConvLayer2D(dim, dim, 3, padding=1, groups=dim, bn_weight_init=0, act_layer = None)\n",
        "\n",
        "        self.ffn = FFN(in_dim=dim, dim=int(dim * mlp_ratio))\n",
        "\n",
        "        #LayerScale\n",
        "        self.alpha = nn.Parameter(1e-4 * torch.ones(4,dim), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        alpha = torch.sigmoid(self.alpha).view(4,-1,1,1)\n",
        "\n",
        "        # DWconv1\n",
        "        x = (1-alpha[0]) * x + alpha[0] * self.dwconv1(x)\n",
        "\n",
        "        # HSM-SSD\n",
        "        x_prev = x\n",
        "        x, h = self.mixer(self.norm(x.flatten(2)))\n",
        "        x = (1-alpha[1]) * x_prev + alpha[1] * x\n",
        "\n",
        "        # DWConv2\n",
        "        x = (1-alpha[2]) * x + alpha[2] * self.dwconv2(x)\n",
        "\n",
        "        # FFN\n",
        "        x = (1-alpha[3]) * x + alpha[3] * self.ffn(x)\n",
        "        return x, h\n",
        "\n",
        "\n",
        "class EfficientViMStage(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, depth,  mlp_ratio=4.,downsample=None, ssd_expand=1, state_dim=64):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.blocks = nn.ModuleList([\n",
        "            EfficientViMBlock(dim=in_dim, mlp_ratio=mlp_ratio, ssd_expand=ssd_expand, state_dim=state_dim) for _ in range(depth)])\n",
        "\n",
        "        self.downsample = downsample(in_dim=in_dim, out_dim =out_dim) if downsample is not None else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            x, h = blk(x)\n",
        "\n",
        "        x_out = x\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x, x_out, h\n",
        "\n",
        "\n",
        "class EfficientViM(nn.Module):\n",
        "    def __init__(self, in_dim=3, num_classes=1, embed_dim=[128,256,512], depths=[2, 2, 2], mlp_ratio=4., ssd_expand=1, state_dim=[49,25,9], distillation=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_layers = len(depths)\n",
        "        self.num_classes = num_classes\n",
        "        self.distillation =distillation\n",
        "        self.patch_embed = Stem(in_dim=in_dim, dim=embed_dim[0])\n",
        "        PatchMergingBlock = PatchMerging\n",
        "\n",
        "        # build stages\n",
        "        self.stages = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            stage = EfficientViMStage(in_dim=int(embed_dim[i_layer]),\n",
        "                               out_dim=int(embed_dim[i_layer+1]) if (i_layer < self.num_layers - 1) else None,\n",
        "                               depth=depths[i_layer],\n",
        "                               mlp_ratio=mlp_ratio,\n",
        "                               downsample=PatchMergingBlock if (i_layer < self.num_layers - 1) else None,\n",
        "                               ssd_expand=ssd_expand,\n",
        "                               state_dim = state_dim[i_layer])\n",
        "            self.stages.append(stage)\n",
        "\n",
        "        # Weights for multi-stage hidden-state Fusion\n",
        "        self.weights = nn.Parameter(torch.ones(4))\n",
        "        self.norm = nn.ModuleList([\n",
        "            LayerNorm1D(embed_dim[0]),\n",
        "            LayerNorm1D(embed_dim[1]),\n",
        "            LayerNorm1D(embed_dim[2]),\n",
        "            LayerNorm2D(embed_dim[2]),\n",
        "        ])\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Linear(embed_dim[0], num_classes) if num_classes > 0 else nn.Identity(),\n",
        "            nn.Linear(embed_dim[1], num_classes) if num_classes > 0 else nn.Identity(),\n",
        "            nn.Linear(embed_dim[2], num_classes) if num_classes > 0 else nn.Identity(),\n",
        "            nn.Linear(embed_dim[2], num_classes) if num_classes > 0 else nn.Identity()\n",
        "        ])\n",
        "\n",
        "        if distillation:\n",
        "            self.weights_dist = nn.Parameter(torch.ones(4))\n",
        "            self.heads_dist = nn.ModuleList([\n",
        "                nn.Linear(embed_dim[0], num_classes) if num_classes > 0 else nn.Identity(),\n",
        "                nn.Linear(embed_dim[1], num_classes) if num_classes > 0 else nn.Identity(),\n",
        "                nn.Linear(embed_dim[2], num_classes) if num_classes > 0 else nn.Identity(),\n",
        "                nn.Linear(embed_dim[2], num_classes) if num_classes > 0 else nn.Identity()\n",
        "            ])\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, LayerNorm2D):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, LayerNorm1D):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.BatchNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def flops(self, shape=(3, 224, 224)):\n",
        "        supported_ops = {\n",
        "            \"aten::silu\": None,\n",
        "            \"aten::neg\": None,\n",
        "            \"aten::exp\": None,\n",
        "            \"aten::flip\": None,\n",
        "            \"aten::softmax\": None,\n",
        "            \"aten::sigmoid\": None,\n",
        "            \"aten::mul\": None,\n",
        "            \"aten::add\": None,\n",
        "            \"aten::mean\": None,\n",
        "            \"aten::var\": None,\n",
        "            \"aten::sub\": None,\n",
        "            \"aten::sqrt\": None,\n",
        "            \"aten::div\": None,\n",
        "            \"aten::rsub\": None,\n",
        "            \"aten::adaptive_avg_pool1d\": None,\n",
        "        }\n",
        "        import copy\n",
        "        model = copy.deepcopy(self)\n",
        "        model.cuda().eval()\n",
        "\n",
        "        input = torch.randn((1, *shape), device=next(model.parameters()).device)\n",
        "        Gflops, unsupported = flop_count(model=model, inputs=(input,), supported_ops=supported_ops)\n",
        "\n",
        "        del model, input\n",
        "\n",
        "        return sum(Gflops.values()) * 1e9\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        weights = self.weights.softmax(-1)\n",
        "        z = torch.zeros((x.shape[0], self.num_classes), device=x.device)\n",
        "        if self.distillation:\n",
        "            weights_dist = self.weights_dist.softmax(-1)\n",
        "            z_dist = torch.zeros((x.shape[0], self.num_classes), device=x.device)\n",
        "\n",
        "        for i, stage in enumerate(self.stages):\n",
        "            x, x_out, h = stage(x)\n",
        "\n",
        "            h = self.norm[i](h)\n",
        "            h = torch.nn.functional.adaptive_avg_pool1d(h, 1).flatten(1)\n",
        "            z = z + weights[i] * self.heads[i](h)\n",
        "            if self.distillation:\n",
        "                z_dist = z_dist + weights_dist[i] * self.heads_dist[i](h)\n",
        "\n",
        "        x = self.norm[3](x)\n",
        "        x = torch.nn.functional.adaptive_avg_pool2d(x, 1).flatten(1)\n",
        "        z = z + weights[3] * self.heads[3](x)\n",
        "\n",
        "        if self.distillation:\n",
        "            z_dist = z_dist + weights_dist[3] * self.heads_dist[3](x)\n",
        "            z= z, z_dist\n",
        "            if not self.training:\n",
        "                z = (z[0] + z[1]) / 2\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "@register_model\n",
        "def EfficientViM_M1(pretrained=False, **kwargs):\n",
        "    model = EfficientViM(\n",
        "        in_dim=3,\n",
        "        embed_dim=[128,192,320],\n",
        "        depths=[2,2,2],\n",
        "        mlp_ratio=4.,\n",
        "        ssd_expand=1.,\n",
        "        state_dim=[49,25,9],\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def EfficientViM_M2(pretrained=False, **kwargs):\n",
        "    model = EfficientViM(\n",
        "        in_dim=3,\n",
        "        embed_dim=[128,256,512],\n",
        "        depths=[2,2,2],\n",
        "        mlp_ratio=4.,\n",
        "        ssd_expand=1.,\n",
        "        state_dim=[49,25,9],\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def EfficientViM_M3(pretrained=False, **kwargs):\n",
        "    model = EfficientViM(\n",
        "        in_dim=3,\n",
        "        embed_dim=[224,320,512],\n",
        "        depths=[2,2,2],\n",
        "        mlp_ratio=4.,\n",
        "        ssd_expand=1.,\n",
        "        state_dim=[49,25,9],\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def EfficientViM_M4(pretrained=False, **kwargs):\n",
        "    model = EfficientViM(\n",
        "        in_dim=3,\n",
        "        embed_dim=[224,320,512],\n",
        "        depths=[3,4,2],\n",
        "        mlp_ratio=4.,\n",
        "        ssd_expand=1.,\n",
        "        state_dim=[64,32,16],\n",
        "        **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L1_ixEwgLFO"
      },
      "outputs": [],
      "source": [
        "# Make modified ViMamba\n",
        "new_vim_pneumnist=EfficientViM_M1(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueCiypKggmkf",
        "outputId": "fc109454-27d5-43a8-c7b5-945c6abbb6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Import in the ViT\n",
        "!pip install einops\n",
        "\n",
        "from torch import nn\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
        "                                               num_heads=n_heads,\n",
        "                                               dropout=dropout)\n",
        "        self.q = torch.nn.Linear(dim, dim)\n",
        "        self.k = torch.nn.Linear(dim, dim)\n",
        "        self.v = torch.nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attn_output, attn_output_weights = self.att(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "from einops import repeat\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, ch=3, img_size=224, patch_size=16, emb_dim=32,\n",
        "                n_layers=6, out_dim=1, dropout=0.1, heads=2):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.channels = ch\n",
        "        self.height = img_size\n",
        "        self.width = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patching\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
        "                                              patch_size=patch_size,\n",
        "                                              emb_size=emb_dim)\n",
        "        # Learnable params\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, emb_dim))\n",
        "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            transformer_block = nn.Sequential(\n",
        "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
        "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
        "            self.layers.append(transformer_block)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Get patch embedding vectors\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # Add cls token to inputs\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "\n",
        "        # Output based on classification token\n",
        "        return self.head(x[:, 0, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5kuH_6hAct",
        "outputId": "71b25133-fb1b-4137-df55-87ec3a3af11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit-pytorch in /usr/local/lib/python3.11/dist-packages (1.9.2)\n",
            "Requirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (0.8.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (0.20.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->vit-pytorch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->vit-pytorch) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->vit-pytorch) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install vit-pytorch\n",
        "\n",
        "import torch\n",
        "from vit_pytorch import SimpleViT\n",
        "\n",
        "new_vit_pneumnist = SimpleViT(\n",
        "    image_size = 224,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84wYAzHAWmoz"
      },
      "outputs": [],
      "source": [
        "# Import needed libraries, torch for nn, numpy for numerical stuff, and plt for quick graphs.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXnGpVZYY1jI"
      },
      "outputs": [],
      "source": [
        "# Create a function for clearning gpu memory\n",
        "import gc\n",
        "\n",
        "def clear_gpu():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXo7Dnn7HEnr"
      },
      "outputs": [],
      "source": [
        "# Help with memory usage.\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvJn2m0YXQP5"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd8sMbPcUPl0",
        "outputId": "9d46a2ea-5ea5-4a3c-92f9-cdc38dd02b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.1.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.20.1+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.1.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efb9tY-lXr9t"
      },
      "outputs": [],
      "source": [
        "# Import PneuMNIST, which we will be training on.\n",
        "\n",
        "from medmnist import PneumoniaMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCr150JOZtpi"
      },
      "outputs": [],
      "source": [
        "# Import needed nn and data tools that just streamline the flow of things.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sil7APSTZiWx"
      },
      "outputs": [],
      "source": [
        "# Define our tensor transforms to our PneuMNIST dataset.\n",
        "\n",
        "download_transform=transforms.Compose([\n",
        "    # Create 3 channels from one just in case\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbC5G2iyPMoX",
        "outputId": "238b18d6-6849-4da1-ecae-58557e723f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist_224.npz\n",
            "train download done!\n",
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist_224.npz\n",
            "test download done!\n"
          ]
        }
      ],
      "source": [
        "# Download Train and Test PneuMNSIT, this is around 224 px which is good for the ViT\n",
        "\n",
        "pneu_mnist_train=PneumoniaMNIST(split='train', download=True, size=224, transform=download_transform)\n",
        "print('train download done!')\n",
        "pneu_mnist_test=PneumoniaMNIST(split='test', download=True, size=224, transform=download_transform)\n",
        "print('test download done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9_piruscEF5"
      },
      "outputs": [],
      "source": [
        "# Create the test and train datasets, REMEMBER BATCH SIZE!\n",
        "\n",
        "test_pneumnist_dataloader=data.DataLoader(dataset=pneu_mnist_test, batch_size=32, num_workers=1, shuffle=False)\n",
        "train_pneumnist=data.DataLoader(dataset=pneu_mnist_train, batch_size=32, num_workers=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "473jGV56P69u"
      },
      "outputs": [],
      "source": [
        "# Delete old torch datasets now that dataloaders are created.\n",
        "\n",
        "del pneu_mnist_train\n",
        "del pneu_mnist_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANstE9dZnHP9"
      },
      "source": [
        "## DENOISING CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoJu2T6snIcw",
        "outputId": "12679e40-0bf1-4584-9400-341de1ffbc99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Setting up our device to train the nn on.\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGH4np6LqnZB"
      },
      "outputs": [],
      "source": [
        "# Creating our denoising CNN.\n",
        "\n",
        "class DenoiseCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DenoiseCNN, self).__init__()\n",
        "    # in layer\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, bias=False)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    # hidden layers\n",
        "    hidden_layers = []\n",
        "    for i in range(18):\n",
        "      hidden_layers.append(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False))\n",
        "      hidden_layers.append(nn.BatchNorm2d(64))\n",
        "      hidden_layers.append(nn.ReLU(inplace=True))\n",
        "    self.mid_layer = nn.Sequential(*hidden_layers)\n",
        "    # out layer\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu1(self.conv1(x))\n",
        "    out = self.mid_layer(out)\n",
        "    out = self.conv3(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deImYjbjsnz2"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "# Create our initial model (WILL BE LOADED IN LATER)\n",
        "denoising_cnn_official=DenoiseCNN().to(device)\n",
        "\n",
        "# Create loss fn and move it to the device.\n",
        "criterion_dcnn=nn.MSELoss()\n",
        "criterion_dcnn=criterion_dcnn.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "lr=1e-4\n",
        "optimizer_dcnn=optim.RMSprop(denoising_cnn_official.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA7doKxjPISo"
      },
      "outputs": [],
      "source": [
        "# Function for adding photon noise(which is what you would expect in a CT or MRI Scanner)\n",
        "class AddPoissonNoise(object):\n",
        "    def __init__(self, mean=0., std=.2):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        tensor=tensor.to(device)\n",
        "        # For example, scale to 255 if the image is in [0, 1] range.\n",
        "        scaled_image = tensor * 255\n",
        "\n",
        "        # Convert to integer for Poisson sampling (photon counts)\n",
        "        scaled_image_int = torch.round(scaled_image).long()\n",
        "\n",
        "        # Generate Poisson noise based on the intensity (scaled_image_int)\n",
        "        noisy_image_int = torch.poisson(abs(scaled_image_int.float()))\n",
        "\n",
        "        # Scale the noisy image back to [0, 1]\n",
        "        noisy_image = noisy_image_int.float() / 255\n",
        "\n",
        "        # Ensure the noisy image is in the valid range [0, 1]\n",
        "        noisy_image = torch.clamp(noisy_image, 0.0, 1.0).to(device)\n",
        "\n",
        "        return noisy_image\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ICmBzafxn92"
      },
      "outputs": [],
      "source": [
        "#training loop\n",
        "def train_dcnn(model, train_loader, criterion, optimizer, epochs_dcnn=15):\n",
        "  model.train()\n",
        "  # Define first loop\n",
        "  for epoch in range(epochs_dcnn):\n",
        "    running_loss=0.0\n",
        "    # Define Second Loop\n",
        "    for images, _ in train_loader:\n",
        "      # Move data to device\n",
        "      images = images.to(device)\n",
        "      #Set optimizer to zero grad and make noisy image\n",
        "      optimizer.zero_grad()\n",
        "      noise_image=AddPoissonNoise().__call__(images).to(device)\n",
        "      # Forward Pass\n",
        "      output=model(noise_image)\n",
        "      loss=criterion(output, images)\n",
        "\n",
        "      # Backward Pass and opt\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Get running loss\n",
        "      running_loss+=loss.item()\n",
        "\n",
        "      # Save model\n",
        "      torch.save(model, \"dcnn.pth\")\n",
        "    # Print loss\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs_dcnn} - Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi5-Zca3RrNv"
      },
      "outputs": [],
      "source": [
        "# Import math modeule for psnr\n",
        "from math import log10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRASAtflQ7kZ"
      },
      "outputs": [],
      "source": [
        "# Evaluate the DCNN and show PSNR if possible :|\n",
        "\n",
        "def test_dcnn(model, test_loader, criterion):\n",
        "  avg_psnr=0.0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "      # Set up target and noisy image\n",
        "      target=images\n",
        "      noise_target=AddPoissonNoise().__call__(images)\n",
        "\n",
        "      target=target.to(device)\n",
        "      noise_target=noise_target.to(device)\n",
        "\n",
        "      output=model(noise_target)\n",
        "      mse=criterion(output, target)\n",
        "      psnr= 10 * log10(1 / mse.item())\n",
        "  print(\"===> Avg. PSNR with 10 * log10(1/mse.item()): {:.4f} dB\".format(avg_psnr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev9tT1H7-f0m"
      },
      "source": [
        "# !!! LOAD IN THE MODEL WHEN NEEDED !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbxFy6c-fMh"
      },
      "outputs": [],
      "source": [
        "# Here, you can load in the model from the pth file provided...right?\n",
        "\n",
        "denoising_cnn_official=torch.load(\"denoising_cnn_official.pth\", weights_only=False, map_location=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxcXE-M05V3K"
      },
      "source": [
        "## ViM PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CToLORSwpMn3"
      },
      "source": [
        "# Combine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-cH1bPgQNaK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CombinedVDCNN(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.dcnn=denoising_cnn_official\n",
        "    self.model=model\n",
        "  def forward(self, x):\n",
        "    x=self.dcnn(x)\n",
        "    x=self.model(x)\n",
        "    return x\n",
        "\n",
        "combvitdcnn=CombinedVDCNN(new_vit_pneumnist)\n",
        "combvimdcnn=CombinedVDCNN(new_vim_pneumnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giyXwKOMRjwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the model with multiple transformer blocks\n",
        "class ComplexModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexModel, self).__init__()\n",
        "\n",
        "        # Define the initial fully connected layers\n",
        "        self.fc1 = nn.Linear(2, 64)   # First layer: 2 inputs -> 64 neurons\n",
        "        self.fc2 = nn.Linear(64, 128) # Second layer: 64 neurons -> 128 neurons\n",
        "\n",
        "        # Transformer block (with more layers)\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=128, nhead=4)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=1)  # You can keep this as 1 layer\n",
        "\n",
        "        # Additional fully connected layers after transformer\n",
        "        self.fc3 = nn.Linear(128, 64) # Third layer: 128 neurons -> 64 neurons\n",
        "        self.fc4 = nn.Linear(64, 1)   # Output layer: 64 neurons -> 1 output\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()   # Sigmoid activation for the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through each layer before transformer\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "\n",
        "        # Transformer expects input in (seq_len, batch_size, model_dim) format\n",
        "        # Add an extra dimension for sequence length (seq_len = 1000)\n",
        "        batch_size = x.size(0)  # Get the batch size\n",
        "        seq_len = 1000  # Set sequence length to 1000\n",
        "\n",
        "        # Reshape the input tensor to (seq_len, batch_size, model_dim)\n",
        "        x = x.unsqueeze(0).expand(seq_len, batch_size, -1)  # Expand to (1000, batch_size, 128)\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Remove the sequence length dimension\n",
        "        x = x.mean(dim=0)  # Average across the sequence length dimension\n",
        "\n",
        "        # Pass through the remaining fully connected layers\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)  # No activation here for final output\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4VM7OKNpOHe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the Gated MLP Class with Sigmoid for gating\n",
        "class GatedMLPHead(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(GatedMLPHead, self).__init__()\n",
        "\n",
        "        # The gating mechanism: learnable parameters to gate the inputs\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1),  # Single gate for each input (ViT, ViM)\n",
        "            nn.Sigmoid()  # Sigmoid to get weights between 0 and 1\n",
        "        )\n",
        "\n",
        "        # The shared MLP layers after gating\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 224),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(224, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(2, 1),\n",
        "            nn.Softmax(dim=1)  # Output probabilities\n",
        "        )\n",
        "\n",
        "    def forward(self, combined_input):\n",
        "        # Apply the gating mechanism\n",
        "        gate_weights = self.gate(combined_input)  # (batch_size, 1)\n",
        "\n",
        "        # Perform the gated combination (element-wise multiplication)\n",
        "        gated_input = gate_weights * combined_input\n",
        "\n",
        "        # Pass through the MLP\n",
        "        output = self.mlp(gated_input)\n",
        "        return output\n",
        "\n",
        "# Create the combination of ViMamba and the ViT\n",
        "class CombinedViTM(nn.Module):\n",
        "    def __init__(self, vit, vim):\n",
        "        super(CombinedViTM, self).__init__()\n",
        "\n",
        "        # Establish our ViMamba and ViT INSIDE the combined model\n",
        "        self.vit = vit\n",
        "        self.vim = vim\n",
        "\n",
        "        # Use the Gated MLP Head instead of the original MLP\n",
        "        self.head = GatedMLPHead(input_dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process the results from both the ViMamba and the ViT\n",
        "        vit_result = self.vit(x)  # Assume output shape (batch_size, 1)\n",
        "        vim_result = self.vim(x)  # Assume output shape (batch_size, 1)\n",
        "\n",
        "        # Concatenate the results from ViT and ViMamba\n",
        "        combined_result = torch.cat((vit_result, vim_result), dim=1)\n",
        "\n",
        "        # Process through the gated MLP head\n",
        "        result = self.head(combined_result)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REz-yeDp_ows"
      },
      "outputs": [],
      "source": [
        "# Create the final model, with the Denoising CNN and merged ViT and ViMamba complete!\n",
        "\n",
        "class done(nn.Module):\n",
        "  def __init__(self, dcnn, combined):\n",
        "    super(done, self).__init__()\n",
        "    # Initialize the models\n",
        "    self.dcnn=dcnn\n",
        "    self.combined=combined\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Simple enough!\n",
        "    x=self.dcnn(x)\n",
        "    x=self.combined(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQvbPuM19a2t"
      },
      "source": [
        "# Theoretical is done..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TI4A1lR3fUM"
      },
      "source": [
        "## IMPORT IN THE Comb!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdOJrLCXAZHu"
      },
      "outputs": [],
      "source": [
        "# Combine\n",
        "combined_pneumnist=CombinedViTM(new_vit_pneumnist, new_vim_pneumnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJZBZLrVgE3G"
      },
      "outputs": [],
      "source": [
        "# Done\n",
        "done_pneumnist=done(denoising_cnn_official, combined_pneumnist)\n",
        "done_pneumnist=done_pneumnist.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rfOANsQLVjt",
        "outputId": "1cd260d8-a1f7-4a98-8d16-05f9fd9818ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59934559"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sum(p.numel() for p in done_pneumnist.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIryPXxrCvLk"
      },
      "outputs": [],
      "source": [
        "# Create loss function and optimizer\n",
        "loss_fn=torch.nn.BCEWithLogitsLoss()\n",
        "loss_fn_alt=torch.nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyVqjR1t7c77"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train_singular(model, train_loader, criterion, optimizer,  loss_list, acc_list, path=\"\", epochs=125):\n",
        "  for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        images, labels=images.to(device), labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        labels = labels.float().view(-1, 1)  # Reshape labels to match output\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track accuracy\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()  # Convert sigmoid output to binary prediction\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.20f}%\")\n",
        "    loss_list.append(epoch_loss)\n",
        "    acc_list.append(epoch_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma18Oa2QBsov"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train_combined(model, train_loader, criterion, optimizer, epochs=100):\n",
        "  for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        images, labels=images.to(device), labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images).float()\n",
        "        loss = criterion(outputs, labels.float())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track accuracy\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()  # Convert sigmoid output to binary prediction\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.25f}, Accuracy: {epoch_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGRn66ORFt5a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "done_pneumnist_predicted=torch.Tensor([])\n",
        "real=torch.Tensor([])\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn, real, done_pneumnist_predicted):\n",
        "  # Evaluate the model\n",
        "  model.eval()  # Set model to evaluation mode\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in dataloader:\n",
        "          images, labels=images.to(device),labels.to(device)\n",
        "          outputs = model(images)\n",
        "          labels = labels.float().view(-1, 1)\n",
        "          real=real.to(device)\n",
        "          done_pneumnist_predicted=done_pneumnist_predicted.to(device)\n",
        "          real=torch.cat((real, labels), dim=0).cuda()\n",
        "          predicted = (outputs > 0.5).float()  # Convert sigmoid output to binary prediction\n",
        "          done_pneumnist_predicted=torch.cat((done_pneumnist_predicted, predicted), dim=0).cuda()\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "  print(f\"real: {real}, predicted={done_pneumnist_predicted}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2hCfa73GgSG"
      },
      "outputs": [],
      "source": [
        "clear_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlMC-vq44FiL"
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeTL25Xe7AAB"
      },
      "outputs": [],
      "source": [
        "optimizer_vit=torch.optim.Adadelta(new_vit_pneumnist.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_IhmtKj7UfQ"
      },
      "outputs": [],
      "source": [
        "optimizer_vim=torch.optim.Adadelta(new_vim_pneumnist.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofSwtX1LsbC5"
      },
      "source": [
        "# LET'S USE LIGHTNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a33EoXE8uk6t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_binary_classification(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.squeeze(), labels.float())  # Squeeze outputs to match labels\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        # Calculate average training loss and accuracy for the epoch\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_accuracy = correct_train / total_train\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxxSeAf5Si_k"
      },
      "outputs": [],
      "source": [
        "combvitdcnn_opt=torch.optim.Adadelta(combvitdcnn.parameters(), lr=0.01)\n",
        "combvimdcnn_opt=torch.optim.Adadelta(combvimdcnn.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E5ZHibt73b4"
      },
      "outputs": [],
      "source": [
        "vit_loss_list=[]\n",
        "vit_acc_list=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eybNkPKA4px"
      },
      "outputs": [],
      "source": [
        "clear_gpu()\n",
        "vim_loss_list=[]\n",
        "vim_acc_list=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfgOkFKci8Yy"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRQIwpki76-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53233c3-bf2b-4459-99d5-ab3010fab19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "head = ComplexModel()\n",
        "head = head.to(device)\n",
        "optimizer_head = torch.optim.Adadelta(head.parameters(), lr=0.01)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, dcnn, vit, vim, head):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        # Assign each module to instance variables\n",
        "        self.dcnn = dcnn\n",
        "\n",
        "        # Convolution layers to increase channels from 3 to 64\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Additional convolutions to bring channels back to 3\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.vit = vit\n",
        "        self.vim = vim\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through dcnn\n",
        "        x = self.dcnn(x)\n",
        "        torch.cuda.empty_cache()\n",
        "        # Apply convolutions to transform channels from 3 -> 64 -> 3\n",
        "        x = self.conv1(x)  # 3 -> 16\n",
        "        torch.cuda.empty_cache()\n",
        "        x = self.conv2(x)  # 16 -> 32\n",
        "        torch.cuda.empty_cache()\n",
        "        x = self.conv3(x)  # 32 -> 64\n",
        "        torch.cuda.empty_cache()\n",
        "        x = self.conv4(x)  # 64 -> 32\n",
        "        torch.cuda.empty_cache()\n",
        "        x = self.conv5(x)  # 32 -> 16\n",
        "        torch.cuda.empty_cache()\n",
        "        x = self.conv6(x)  # 16 -> 3\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Forward pass through vit and vim\n",
        "        vit_out = self.vit(x)\n",
        "        torch.cuda.empty_cache()\n",
        "        vim_out = self.vim(x)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Concatenate the outputs of vit and vim along the feature dimension (dim=1)\n",
        "        x = torch.cat((vit_out, vim_out), dim=1)\n",
        "\n",
        "        # Pass the concatenated output to the head\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "c=CustomModel(denoising_cnn_official, new_vit_pneumnist, new_vim_pneumnist, head)\n",
        "c.load_state_dict(torch.load(\"c_sd.pth\", weights_only=True), strict=False)\n",
        "c=c.to(device)\n",
        "optimizer_c=torch.optim.Adadelta(c.parameters(), lr=0.01)\n",
        "vit_loss_list=[]\n",
        "vit_acc_list=[]\n",
        "train_singular(c, train_pneumnist, loss_fn, optimizer_c, vit_loss_list, vit_acc_list, epochs=14)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(c, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ],
      "metadata": {
        "id": "61QgT02W1iYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class NpzDataset(Dataset):\n",
        "    def __init__(self, npz_file_path, img, label, transform=None,):\n",
        "        # Load the .npz file\n",
        "        data = np.load(npz_file_path)\n",
        "\n",
        "        # Assuming the .npz file contains arrays named 'images' and 'labels'\n",
        "        self.images = data['train_images']\n",
        "        self.labels = data['train_labels']\n",
        "\n",
        "        # Set up the transform\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get an image and label\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert image from NumPy array to PIL image (required for torchvision transforms)\n",
        "        image = Image.fromarray(image.astype('uint8'))  # Convert to uint8 for compatibility\n",
        "\n",
        "        # Apply transform if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor (for classification task, long for labels)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
        "    transforms.ToTensor(),  # Convert image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "# Initialize the dataset with transformations\n",
        "npz_file_path = '/content/breastmnist_224.npz'  # Path to your .npz file\n",
        "dataset_train = NpzDataset(npz_file_path, \"train_images\", \"train_labels\", transform=transform)\n",
        "dataset_test = NpzDataset(npz_file_path, \"test_images\", \"test_labels\", transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "breastmnist_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
        "breastmnist_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
        "\n",
        "train_singular(c, breastmnist_train, loss_fn, optimizer_c, vit_loss_list, vit_acc_list, epochs=14)"
      ],
      "metadata": {
        "id": "CoZ7fJNLc604"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "evaluate(c, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ],
      "metadata": {
        "id": "wMFwCC-defN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(c.state_dict(), \"c_sd.pth\")"
      ],
      "metadata": {
        "id": "3w3rjjOEHwez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in done_pneumnist.parameters() if p.requires_grad)+sum(p.numel() for p in head.parameters())"
      ],
      "metadata": {
        "id": "nfhZe5rkcKdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOYqkSrKnyRT"
      },
      "source": [
        "# TEST OUT OTHER MODELS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(model):\n",
        "  print(sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "4plHeKT0dnsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Bv_bAAjOYN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_params(model)"
      ],
      "metadata": {
        "id": "vt-Qa3oidwJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_IEWq2BjV9A"
      },
      "outputs": [],
      "source": [
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlLkHGzHkKDm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet34(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m7CkxA4j0E3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z48msAPHkF3g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet101(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m9vPzs6kV3z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet152(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OPl3DVGneKX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "model.classifier[6] = torch.nn.Linear(4096, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fldxOydKning"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "model\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "model.classifier[3] = torch.nn.Linear(1280, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afJGa-lmuuGB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.densenet201(pretrained=True)\n",
        "\n",
        "model\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "model.classifier = torch.nn.Linear(1920, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, test_pneumnist_dataloader, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imak7P7OtfQY"
      },
      "source": [
        "# TEST OUT OTHER MODELS BCANCER"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(model):\n",
        "  print(sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "1T9qJm5YtfQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faKnMlGltfQZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_params(model)"
      ],
      "metadata": {
        "id": "vTAtWEHotfQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fTauz7UtfQZ"
      },
      "outputs": [],
      "source": [
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaFEXg9TtfQa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet34(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4t18EY1tfQa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9EDGOB_tfQa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet101(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwSZoY5btfQa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet152(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQsQSqTHtfQb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "model.classifier[6] = torch.nn.Linear(4096, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWMN5-ChtfQb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "model\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "model.classifier[3] = torch.nn.Linear(1280, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn4yXLcetfQb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.densenet201(pretrained=True)\n",
        "\n",
        "model\n",
        "# If you want to freeze all the layers except the final fully connected layer:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match your number of classes\n",
        "model.classifier = torch.nn.Linear(1920, 1) # Replace 10 with your number of classes\n",
        "\n",
        "# Move the model to the desired device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "count_params(model)\n",
        "\n",
        "evaluate(model, breastmnist_test, loss_fn, real, done_pneumnist_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_new=CustomModel(denoising_cnn_official, new_vit_pneumnist, new_vim_pneumnist, head)\n",
        "c_new.load_state_dict(torch.load(\"c_sd.pth\", weights_only=True))"
      ],
      "metadata": {
        "id": "qKgayXlAuFV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}